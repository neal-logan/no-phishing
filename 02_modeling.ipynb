{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neal-logan/dsba6211-summer2024/blob/main/nophishing/02_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No Phishing: Detecting Malicious URLs\n",
        "## Notebook 02: Models and Evaluation"
      ],
      "metadata": {
        "id": "krTfazTvLq59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup"
      ],
      "metadata": {
        "id": "Dd79hjmLfSut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Install required packages"
      ],
      "metadata": {
        "id": "Srypb7dDfplB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Docs: https://github.com/facebookresearch/hiplot\n",
        "%%capture\n",
        "!pip install hiplot==0.1.33"
      ],
      "metadata": {
        "id": "Q7sjZBEKfU6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Docs: https://github.com/SelfExplainML/PiML-Toolbox\n",
        "%%capture\n",
        "!pip install PiML==0.6.0"
      ],
      "metadata": {
        "id": "OMR0NpunfVV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set random seed"
      ],
      "metadata": {
        "id": "KJ7tnAK9fmq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42"
      ],
      "metadata": {
        "id": "fXkTPBfvfzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Data Preparation\n",
        "\n",
        "### Overview of the Dataset\n",
        "\n",
        "First, a quick review of the data explored in the previous notebook.  The dataset consistes of the raw URL, the binary phishing label, and 87 features, including:\n",
        "* 56 from URL syntax and structure,\n",
        "* 24 from corresponding site content, and\n",
        "* 7 from external services.\n",
        "\n",
        "### Preprocessing Steps\n",
        "\n",
        "Prior to beginning the preprocessing pipeline, the following transformations are made:\n",
        "* Transform the target variable **status** to binary, where 1 = 'phishing' and 0 = 'legitimate', and rename the column to **y**\n",
        "* Drop the raw URL, as the modeling techniques can only make effective use of quantitative features.\n",
        "* Drop all columns identified as unnecessary during exploratory data analysis\n",
        "* Split the **training** dataset into X and y divisions (separating the features from the target variable), and each of these into training and validation data frames (the original dataset did not contain a separate validation set).  \n",
        "\n",
        "Note: The **test** dataset will not be loaded until final model evaluation.\n",
        "\n",
        "### Selected Features\n",
        "\n",
        "Based on analysis conducted in [01 Exploratory Analysis](https://github.com/neal-logan/dsba6211-summer2024/blob/b3308e42ffb44ffdf364b85520f4af9c19ece147/nophishing/01_exploratory_analysis.ipynb), 14 features were selected. Roughly in order of importance, these selected features included:\n",
        "* google_index\n",
        "* page_rank\n",
        "* nb_www\n",
        "* nb_hyperlinks\n",
        "* domain_age\n",
        "* phish_hints\n",
        "* nb_hyphens\n",
        "* nb_qm\n",
        "* web_traffic\n",
        "* ratio_digits_host\n",
        "* nb_dots\n",
        "* length_words_raw\n",
        "* nb_slash\n",
        "* ratio_extHyperlinks\n",
        "\n",
        "### Contextual Notes on Feature Selection\n",
        "\n",
        "The importance of these features drops off very quickly, with the most important feature (google_index) around 50 times more important than the least-important feature. The importance threshold was set as low as it was because accuracy tends to need to be emphasized in this context, and also can be, for a few reasons.\n",
        "\n",
        "The difference between 96.0% precision or recall and, say, 96.4% may not seem like much, but at the margin it's a 10% reduction in prevented attacks or false positives.  The inconvenience of a message incorrectly blocked can be substantial, and the damage caused by a succesful attack can be catastrophic.  Social engineering schemes like phishing are among the most dangerous and difficult threats to address, and in many cases will not be detected even security experts.\n",
        "\n",
        "At the same time, models used to detect fraud attempts or raise other pressing security concerns in a high-velocity, high-volume context like messaging should be (and typically will be) monitored closely and updated regularly as threat actors' tactics change.  If a model like this becomes slightly outdated due to changes in TA tactics, the problem can be rectified promptly--as long as labels are promptly obtained, in this case mainly from messaging users."
      ],
      "metadata": {
        "id": "B05NXjQnMMqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgq7lpI8P335"
      },
      "outputs": [],
      "source": [
        "# Load training dataset (this will be divided into train and validation sets)\n",
        "import pandas as pd\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/neal-logan/dsba6211-summer2024/main/nophishing/data/phishing-url-pirochet-train.csv'\n",
        "df = pd.read_csv(train_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 Features selected through exploratory data analysis\n",
        "\n",
        "selected_features = [\n",
        "    'google_index', 'page_rank', 'nb_www', 'nb_hyperlinks', 'domain_age',\n",
        "    'phish_hints', 'nb_hyphens', 'nb_qm', 'web_traffic', 'ratio_digits_host',\n",
        "    'nb_dots', 'length_words_raw', 'nb_slash', 'ratio_extHyperlinks']\n"
      ],
      "metadata": {
        "id": "yrfCocbLnGnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create numeric target variable column\n",
        "df['y'] = df['status'].replace('legitimate', 0).replace('phishing', 1)\n",
        "\n",
        "#Drop unnecessary columns, retaining only selected features and the target var\n",
        "df = df.filter(selected_features + ['y'])\n",
        "\n",
        "#Convert invalid values to show they are missing\n",
        "import numpy as np\n",
        "df['domain_age'] = [np.NaN if x < 0 else x for x in df['domain_age']]"
      ],
      "metadata": {
        "id": "eUAxk_dlvl9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature/target split\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']"
      ],
      "metadata": {
        "id": "TD2W52JdvkKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training/validation split (test data not yet loaded)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = random_seed)"
      ],
      "metadata": {
        "id": "vkKZ6DM3TGKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development\n",
        "\n",
        "### Modeling Methodology\n",
        "\n",
        "An approach using sklearn's histogram-based gradient-boosted decision tree classifier was selected for the following reasons:\n",
        "* **Accuracy**: GBTs tend to be very accurate; more accurate than random forest, and sometimes a great deal more accurate than logistic regression, although the differences were not overwhelming for this problem\n",
        "* **Robustness**: HGBTs can handle outliers and missing data gracefully\n",
        "* **Flexbility**: HGBTs can capture non-linear relationships, and sklearn's HGBT [handles categorical variables natively](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_categorical.html), without special encoding\n",
        "* **Explainability**: GBTs are reasonably transparent and explainable\n",
        "* **Scalability**: HGBTs can dramatically outperform non-histogram GBTs in training speed due to bucketing, especially at scale\n",
        "\n",
        "### Model Risk\n",
        "\n",
        "Key model risk concerns include:\n",
        "* Overfitting and underfitting\n",
        "* Exploitable weakspots, and\n",
        "* A lack of resilience to shifts in the distributions of legitimate activity or threat actor tactics."
      ],
      "metadata": {
        "id": "gJBhW2Q4DA3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Baseline Model\n",
        "\n",
        "\n",
        "* **Histogram gradient-booosted tree model: ROC-AUC > 0.96**\n",
        " * Iterations limted to 100 (default 100, increased from 60)\n",
        " * Tree depth limited to 8 (default none, increased from 7)\n",
        " * Both parameters selected to simplify model and avoid overfitting, but increased slightly from EDA model to address undertraining\n",
        " * StandardScaler() removed as unnecessary for histogram-based GBT\n",
        " * Overfitting concerns minimal"
      ],
      "metadata": {
        "id": "0yMepv8vP5OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Define model evaluation function\n",
        "\n",
        "def print_model_evaluation(\n",
        "    title: str,\n",
        "    pipe : Pipeline,\n",
        "    X : pd.DataFrame,\n",
        "    y : pd.DataFrame):\n",
        "\n",
        "    print(\"\\n\" + title)\n",
        "    pred_y = pipe.predict(X)\n",
        "    print(confusion_matrix(pred_y, y))\n",
        "    print(\"\\nROC-AUC: \" + str(roc_auc_score(pred_y, y)))\n",
        "    print(\"Precision: \" + str(precision_score(pred_y, y)))\n",
        "    print(\"Recall: \" + str(recall_score(pred_y, y)))\n"
      ],
      "metadata": {
        "id": "VKh5JS7xQ2Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and run pipeline - gradient boosted trees\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_gbt = make_pipeline(\n",
        "      HistGradientBoostingClassifier(\n",
        "          max_iter=100,\n",
        "          max_depth= 8,\n",
        "          random_state=random_seed)\n",
        ")\n",
        "\n",
        "pipe_gbt.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PDaZk9gZRdC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Training Set\",\n",
        "                       pipe_gbt, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Validation Set\",\n",
        "                       pipe_gbt, X_validation, y_validation)"
      ],
      "metadata": {
        "id": "YLAsZvt6RllN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PiML Experment Setup\n",
        "\n",
        "The experiment was set up using the combined training and validation sets.  The performance metrics are suspicious, with accuracy at 0.96 yet AUC and F1 both greater than 0.99 on the 'test' (experiment-internal validation) set.\n"
      ],
      "metadata": {
        "id": "Io3q-xsj7ciM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from piml import Experiment"
      ],
      "metadata": {
        "id": "89XbDzNTyN6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(highcode_only=True)"
      ],
      "metadata": {
        "id": "KQn0zPYAyNm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.data_loader(data = df)"
      ],
      "metadata": {
        "id": "y-Y4U4WJ3PxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.data_prepare(target = 'y', task_type = 'classification', silent=True)"
      ],
      "metadata": {
        "id": "7U0NNqxu47CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train basic histogram GBT model"
      ],
      "metadata": {
        "id": "74jY7O4W95cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_train(\n",
        "    model = HistGradientBoostingClassifier(\n",
        "        max_iter=100,\n",
        "        max_depth=8,\n",
        "        random_state=random_seed),\n",
        "    name = 'hgbt')\n"
      ],
      "metadata": {
        "id": "MwyjVFCd3Pn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(model = 'hgbt', show='accuracy_table')"
      ],
      "metadata": {
        "id": "xOUc5mrNM_r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Dependency Plots & Monotonicity Assessment\n",
        "\n",
        "Some features are already monotonously well-behaved, for example:\n",
        "* phish_hints\n",
        "* nb_hyphens\n",
        "* nb_dots\n",
        "\n",
        "Others show more complex and concerning patterns:\n",
        "* web_traffic: high at extremely low or high traffic, and noisy in the middle.\n",
        "* ratio_extHyperlinks: looks like a tilted 'W'\n",
        "\n",
        "A few features are nearly monotonous, with some anomalies, suggesting that a monotonous constraint might be appropriate:\n",
        "* page_rank: this feature seems like it pretty clearly should be monotonous\n",
        "* nb_hyperlinks:\n",
        "* ratio_digits_host:\n",
        "* length_words_raw\n",
        "* nb_slash\n"
      ],
      "metadata": {
        "id": "esAQkqM77iRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select feature to vew\n",
        "feature_to_view = 'length_words_raw'\n",
        "\n",
        "#Show partial dependency plot\n",
        "exp.model_explain(\n",
        "    model='hgbt',\n",
        "    show='pdp',\n",
        "    uni_feature=feature_to_view)"
      ],
      "metadata": {
        "id": "sfplvSB302c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model with Monotonicity Constraint\n",
        "\n",
        "Given the results of the monotonicity assessment, a new model was developed with a monotonicity constraint on the five features which appeared to have a monotonous relationship, but which showed anomaloies:\n",
        "* page_rank\n",
        "* nb_hyperlinks\n",
        "* ratio_digits_host\n",
        "* length_words_raw\n",
        "* nb_slash\n"
      ],
      "metadata": {
        "id": "o4tFG_cj-tZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html\n",
        "# This array controls the direction of monotonicity constraints by feature\n",
        "# HistGradientBoostingClassifier's monotonic_cst parameter takes either (1) an\n",
        "# array of the same length as the feature set, like this, or (2) a dictionary\n",
        "# of specific features and the monotonicity setting for that feature, but (2)\n",
        "# didn't work in this instance because for some reason, the feature names\n",
        "# didn't pass through to the model. So this less-readable code was unavoidable.\n",
        "# 1 = increasing constraint, -1 = decreasing constraint, 0 = no constraint\n",
        "monotonic_contraints = [\n",
        "    0,-1,0,-1,0,0,0,0,0,1,0,1,1,0\n",
        "]"
      ],
      "metadata": {
        "id": "O0Lc9vuX7G-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model with the same settings as the new baseline HGBT\n",
        "exp.model_train(\n",
        "    model = HistGradientBoostingClassifier(\n",
        "        max_iter=100,\n",
        "        max_depth=8,\n",
        "        monotonic_cst = monotonic_contraints,\n",
        "        random_state=random_seed),\n",
        "    name = 'hgbt_mono')\n"
      ],
      "metadata": {
        "id": "fsbQ5Ga7_KFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy & Monotonicity Comparison\n",
        "\n",
        "Monotonicity constraints were applied while training the new model. Somewhat surprisingly, there was no loss of accuracy on the constrained model--in fact, accuracy improved very slightly on the 'test' (locally-partitioned validation) dataset.\n",
        "\n",
        "Since these models both seem promising, we'll proceed to deeper analysis of them."
      ],
      "metadata": {
        "id": "Yv8KS8gOpauI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(model = 'hgbt', show='accuracy_table')"
      ],
      "metadata": {
        "id": "5lebSTr-KcHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(model = 'hgbt_mono', show='accuracy_table')"
      ],
      "metadata": {
        "id": "P1CVtV3PKUAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select feature to vew\n",
        "feature_to_view = 'nb_hyperlinks'"
      ],
      "metadata": {
        "id": "aFdYfJ8yJKbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Original model\n",
        "exp.model_explain(\n",
        "    model='hgbt',\n",
        "    show='pdp',\n",
        "    uni_feature=feature_to_view)"
      ],
      "metadata": {
        "id": "55gmw1RrAVn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Monotonistic model\n",
        "exp.model_explain(\n",
        "    model='hgbt_mono',\n",
        "    show='pdp',\n",
        "    uni_feature=feature_to_view)"
      ],
      "metadata": {
        "id": "M5G7zArFIqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting Analysis\n",
        "\n",
        "In the base hgbt model, some mild local overfitting was present in **page_rank** and **nb_slash**, based on individual feature assessments. Mild overfitting on **page_rank** is not surprising, given the non-monotonicity in unconstrained model, and this overfitting is adequately addressed in the monotonically-constrained model. Overfitting on **nb_slash**, however, has not been addressed.\n"
      ],
      "metadata": {
        "id": "AJemnkLTaj7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  if col not in ['domain_age', 'y']:\n",
        "    print('\\n' + col)\n",
        "    results = exp.model_diagnose(\n",
        "        model=\"hgbt\",\n",
        "        show=\"overfit\",\n",
        "        slice_method=\"histogram\",\n",
        "        slice_features=[col],\n",
        "        threshold=1.05,\n",
        "        min_samples=100)\n"
      ],
      "metadata": {
        "id": "1hlaOZBoiF4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  if col not in ['domain_age', 'y']:\n",
        "    print('\\n' + col)\n",
        "    results = exp.model_diagnose(\n",
        "    model=\"hgbt_mono\",\n",
        "    show=\"overfit\",\n",
        "    slice_method=\"histogram\",\n",
        "    slice_features=[col],\n",
        "    threshold=1.05,\n",
        "    min_samples=100)"
      ],
      "metadata": {
        "id": "23fS7QdmiObo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weakspot Analysis\n",
        "\n",
        "All features except domain_age were assessed individually for weak points. The missing values in domain_age seemed to cause problems with the test. Of the individually-tested features, only **ratio_digits_host** showed a weak point, and it was in an area of fairly low density.  \n",
        "\n",
        "Interestingly, this weak point shows up only in the monotonicity-constrainted model."
      ],
      "metadata": {
        "id": "ie5XdpQr-pH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  if col not in ['domain_age', 'y']:\n",
        "    print('\\n' + col)\n",
        "    exp.model_diagnose(\n",
        "      model='hgbt',\n",
        "      show='weakspot',\n",
        "      slice_method='histogram',\n",
        "      slice_features= [col],\n",
        "      threshold=1.1,\n",
        "      min_samples=200,\n",
        "      metric='AUC',\n",
        "      use_test=True)"
      ],
      "metadata": {
        "id": "iXC2dNdeRTYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  if col not in ['domain_age', 'y']:\n",
        "    print('\\n' + col)\n",
        "    exp.model_diagnose(\n",
        "      model='hgbt_mono',\n",
        "      show='weakspot',\n",
        "      slice_method='histogram',\n",
        "      slice_features= [col],\n",
        "      threshold=1.1,\n",
        "      min_samples=200,\n",
        "      metric='AUC',\n",
        "      use_test=True)"
      ],
      "metadata": {
        "id": "36OMlRTwPLv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_explain(\n",
        "    model='hgbt',\n",
        "    show='pdp',\n",
        "    uni_feature='ratio_digits_host')"
      ],
      "metadata": {
        "id": "ye3uMN3ITRYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_explain(\n",
        "    model='hgbt_mono',\n",
        "    show='pdp',\n",
        "    uni_feature='ratio_digits_host')"
      ],
      "metadata": {
        "id": "4dluB91TTM7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resilience Analysis\n",
        "\n",
        "Although we might choose a more accurate model over a more resilient model to some extent in a closely-monitored context, *low* resilience is nevertheless intolerable.\n",
        "\n",
        "Overall, we should expect this model to detect a lot of phishing even in scenarios with a large, sudden shift in legitimate activity or threat actor tactics: accuracy is still over 80% with a worst-case sample ratio of 0.2. Dropping from 96% accuracy to 80% represents a five-fold error, which we can expect to be apparent to both legitimate users and threat actors. A new model would be needed--urgently--but brief, occasional periods of weaker performance like this are typically tolerable.\n",
        "\n",
        "Resilience distance analysis shows that **nb_hyperlinks**, **page_rank**, and other important features would contain minor vulnerabilities to change in distribution in the base model, but that this vulnerability is substantially reduced in the monotonicity-constrained model, highlighting a significant advantage."
      ],
      "metadata": {
        "id": "1UptyHCcanUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PiML resilience performance"
      ],
      "metadata": {
        "id": "n8Ydxgfxdy9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(\n",
        "      model='hgbt',\n",
        "      show='resilience_perf',\n",
        "      resilience_method='worst-sample')"
      ],
      "metadata": {
        "id": "Xy0uYuc2dr0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(\n",
        "      model='hgbt_mono',\n",
        "      show='resilience_perf',\n",
        "      resilience_method='worst-sample')"
      ],
      "metadata": {
        "id": "Oyfufg48dxxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PiML resilience distance"
      ],
      "metadata": {
        "id": "T8va848cd2oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Considers the effects of a distributional shift to the 20% worst observations,\n",
        "# compared to\n",
        "exp.model_diagnose(\n",
        "      model='hgbt',\n",
        "      show='resilience_distance',\n",
        "      resilience_method='worst-sample',\n",
        "      distance_metric = 'PSI',\n",
        "      alpha=0.2)"
      ],
      "metadata": {
        "id": "SVj9hmNMcm94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.model_diagnose(\n",
        "      model='hgbt_mono',\n",
        "      show='resilience_distance',\n",
        "      resilience_method='worst-sample',\n",
        "      distance_metric = 'PSI',\n",
        "      alpha=0.2)"
      ],
      "metadata": {
        "id": "T6TcikeudPbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model Training & Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "Dtq8OTXcyVF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Data Preparation"
      ],
      "metadata": {
        "id": "RDhWbKPv2_ER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load test data"
      ],
      "metadata": {
        "id": "qUzGvpUoquT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the final test dataset!\n",
        "test_url = 'https://raw.githubusercontent.com/neal-logan/dsba6211-summer2024/main/nophishing/data/phishing-url-pirochet-test.csv'\n",
        "df_test = pd.read_csv(test_url)"
      ],
      "metadata": {
        "id": "Bqs-XHloaXRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preprocess test data"
      ],
      "metadata": {
        "id": "beG--MSl2poV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create numeric target variable column\n",
        "df_test['y'] = df_test['status'].replace('legitimate', 0).replace('phishing', 1)\n",
        "\n",
        "#Drop unnecessary columns, retaining only selected features and the target var\n",
        "df_test = df_test.filter(selected_features + ['y'])\n",
        "\n",
        "#Convert invalid values to show they are missing\n",
        "import numpy as np\n",
        "df_test['domain_age'] = [np.NaN if x < 0 else x for x in df_test['domain_age']]"
      ],
      "metadata": {
        "id": "UmwI1ppc1Oz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature/target split"
      ],
      "metadata": {
        "id": "7CMwxKPw2r5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use both the training and validation sets for final model training to maximize performance\n",
        "X_full_train = df.drop(columns=['y'])\n",
        "y_full_train = df['y']\n",
        "\n",
        "#Feature/target split - TEST\n",
        "X_test = df_test.drop(columns=['y'])\n",
        "y_test = df_test['y']"
      ],
      "metadata": {
        "id": "P7GH7W6b1TYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Modeling & Model Assessment\n",
        "\n",
        "The final model was trained the same way as the monotonicity-constrained model in almost every respect:\n",
        "* used SKleran's histogram gradient-boosting classifier,\n",
        "* set max_iter to 100,\n",
        "* set max_depth to 8,\n",
        "* set monotonic constraints on the same five features, and\n",
        "* used the same random seed.\n",
        "\n",
        "The only deviations were:\n",
        "* training on the combined training and validation datasets and tested against the test set, and\n",
        "* training outside a PiML experiment.\n",
        "\n",
        "Results suggest similar outcomes to the baseline model, as expected:\n",
        "* **ROC-AUC** = **0.962**\n",
        "* Precision = 0.967\n",
        "* Recall = 0.957\n",
        "\n",
        "#### Further testing needed\n",
        "\n",
        "However, further testing should be done to check for potentially overfitting (despite low risk) as well as weakspots and potential resilience issues."
      ],
      "metadata": {
        "id": "g_0fppZ53DjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set up and run model pipeline - monotonically-constrained model"
      ],
      "metadata": {
        "id": "UoM8kYqA2zIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and run pipeline - gradient boosted trees\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_gbt_prod = make_pipeline(\n",
        "      HistGradientBoostingClassifier(\n",
        "          max_iter=100,\n",
        "          max_depth= 8,\n",
        "          monotonic_cst = monotonic_contraints,\n",
        "          random_state=random_seed)\n",
        ")\n",
        "\n",
        "pipe_gbt_prod.fit(X_full_train, y_full_train)"
      ],
      "metadata": {
        "id": "P9g5LlZQ0xiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Analysis"
      ],
      "metadata": {
        "id": "x9VRf_xl3mF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Training Set\",\n",
        "                       pipe_gbt_prod, X_full_train, y_full_train)\n",
        "\n",
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Test Set\",\n",
        "                       pipe_gbt_prod, X_test, y_test)"
      ],
      "metadata": {
        "id": "Tb0riNf802xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}