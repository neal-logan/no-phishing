{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neal-logan/dsba6211-summer2024/blob/main/nophishing/01_exploratory_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RET7C16wNHx6"
      },
      "source": [
        "# No Phishing: Detecting Malicious URLs\n",
        "\n",
        "## Notebook 01: Exploratory Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVYV2jdLoy9"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "Developed with Python version 3.10.12 in Colab version 1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7SzEAhKz6C4"
      },
      "source": [
        "##### Install required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80KczabzVNle"
      },
      "outputs": [],
      "source": [
        "#Docs: https://github.com/facebookresearch/hiplot\n",
        "%%capture\n",
        "!pip install hiplot==0.1.33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1eSP4w8aNc6"
      },
      "outputs": [],
      "source": [
        "#Docs: https://github.com/SelfExplainML/PiML-Toolbox\n",
        "%%capture\n",
        "!pip install PiML==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzAAYv0dElgx"
      },
      "source": [
        "##### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKOK1l8gEnfJ"
      },
      "outputs": [],
      "source": [
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D77WZoKCNQgR"
      },
      "source": [
        "### Overview of the Data\n",
        "\n",
        "The dataset consistes of the raw URL, the binary phishing label, and 87 features.\n",
        "\n",
        "**Category of data source**:  Of the 87 features:\n",
        "* 56 were created from URL syntax and structure,\n",
        "* 24 were extracted from corresponding site content, and\n",
        "* 7 were obtained from third-party services.\n",
        "\n",
        "**Variable types**: The features are a mix of binary, integer, and floating-point variables.\n",
        "\n",
        "**Distributions**: Variables tended to follow a few patters in their variation:\n",
        "* Integer distributions: Some include only small numbers in the range of 1-10 and essentially ordinal categories, such as page_rank.  Others are counts of URL or site features that tend to get little larger than 50, and tend to be right-skewed.  Still others are strongly right-skewed and range into the hundreds or thousands, such as url_length or longest_word_raw.  \n",
        "* Binary distributions: some are fairly well balanced while others show almost no variation at all.  \n",
        "* Floating-point distributions: all floating point variables appear to be ratios between 0 and 1; some of them are distributed throughout that range, while others behave more like binary variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsdvzxeCXAR-"
      },
      "source": [
        "##### Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOS-vzfHNouy"
      },
      "outputs": [],
      "source": [
        "# Load and prepare training data\n",
        "import pandas as pd\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/neal-logan/dsba6211-summer2024/main/nophishing/data/phishing-url-pirochet-train.csv'\n",
        "df = pd.read_csv(train_url)\n",
        "\n",
        "#Create numeric target variable column\n",
        "df['y'] = df['status'].replace('legitimate', 0).replace('phishing', 1)\n",
        "\n",
        "#Drop unnecessary columns\n",
        "df = df.drop(columns=['status','url'])\n",
        "\n",
        "#X/y split\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOc4Zid9VnmV"
      },
      "outputs": [],
      "source": [
        "#Split training set into training and validation set (test set not yet loaded)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgK2sOE5GcJ2"
      },
      "outputs": [],
      "source": [
        "# Establish a list of columns being dropped from exploration and modeling\n",
        "column_drop_list = ['url','status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpiuKe2wUXnh"
      },
      "source": [
        "##### Categorize Features by Source\n",
        "Based on data documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SIDsnEuUdti"
      },
      "outputs": [],
      "source": [
        "# Split features into categories based on origin: from URL, from site content,\n",
        "# or from third parties\n",
        "\n",
        "url_columns = ['length_url', 'length_hostname', 'ip', 'nb_dots',\n",
        "  'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "  'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star',\n",
        "  'nb_colon', 'nb_comma', 'nb_semicolumn', 'nb_dollar', 'nb_space',\n",
        "  'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token',\n",
        "  'ratio_digits_url', 'ratio_digits_host', 'punycode', 'port',\n",
        "  'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain',\n",
        "  'nb_subdomains', 'prefix_suffix', 'random_domain', 'shortening_service',\n",
        "  'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "  'length_words_raw', 'char_repeat', 'shortest_words_raw',\n",
        "  'shortest_word_host', 'shortest_word_path', 'longest_words_raw',\n",
        "  'longest_word_host', 'longest_word_path', 'avg_words_raw',\n",
        "  'avg_word_host', 'avg_word_path', 'phish_hints', 'domain_in_brand',\n",
        "  'brand_in_subdomain', 'brand_in_path', 'suspecious_tld',\n",
        "  'statistical_report' ]\n",
        "\n",
        "site_content_columns = ['nb_hyperlinks', 'ratio_intHyperlinks',\n",
        "  'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "  'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors',\n",
        "  'ratio_extErrors', 'login_form', 'external_favicon', 'links_in_tags',\n",
        "  'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "  'popup_window', 'safe_anchor', 'onmouseover', 'right_clic',\n",
        "  'empty_title', 'domain_in_title', 'domain_with_copyright']\n",
        "\n",
        "third_party_columns = ['whois_registered_domain', 'domain_registration_length', 'domain_age',\n",
        "       'web_traffic', 'dns_record', 'google_index', 'page_rank']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Summary statistics"
      ],
      "metadata": {
        "id": "qX7I_tt84YDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.describe()"
      ],
      "metadata": {
        "id": "8ONBscubC08d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"page_rank\"].describe()\n",
        "\n",
        "print(\"Five-number Summary\\nMin-25%-50%-75%-Max\")\n",
        "for col in X_train.columns:\n",
        "  stats = X_train[col].describe()\n",
        "  print(col + \": \" + \"{:.1f}\".format(stats[\"min\"]) + \" - \" + \"{:.1f}\".format(stats[\"25%\"]) + \" - \" + \"{:.1f}\".format(stats[\"50%\"]) + \" - \" + \"{:.1f}\".format(stats[\"75%\"]) + \" - \" + \"{:.1f}\".format(stats[\"max\"]))"
      ],
      "metadata": {
        "id": "MyK300uI4USB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invalid Feature Values\n",
        "\n",
        "Just two features have apparently invalid values:\n",
        "* domain_registration length has a handful of invalid observations affecting less than 1% of observations, while\n",
        "* domain_age is invalid for 16% of observations; with the invalid values well-balanced between phishing and legitimate.\n",
        "\n",
        "Sklearn's histogram gradient-boosted tree classifiers handle NaNs natively.   Since the main feature importance analysis and (more importantly) the main modeling efforts will use these classifiers, all negative values will be set to NaN.  However, they will not be removed until after baseline modeling, since the logistic regression and random forest classifier models can't handle NaNs."
      ],
      "metadata": {
        "id": "Mr34x2YeqdBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For most features, values less than 0 will be invalid\n",
        "\n",
        "for col in X_train.columns:\n",
        "  invalid_count = X_train[X_train[col] < 0].shape[0]\n",
        "\n",
        "  if(invalid_count > 0):\n",
        "    X_train.shape[0]\n",
        "    invalid_phishing = X_train[(X_train[col] < 0) & (y_train == 1)].shape[0]*1.0 / invalid_count\n",
        "    print(col\n",
        "          + \":\\n   Invalid observations:  \" + str(invalid_count)\n",
        "          + \"\\n   Proportion of observations that are invalid:  \" + str(invalid_count*1.0/X_train.shape[0])\n",
        "          + \"\\n   Proportion of invalids that are phishing:  \" + str(invalid_phishing))\n"
      ],
      "metadata": {
        "id": "Jy94Pukyqch4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grh_DRcvFoeJ"
      },
      "source": [
        "### Feature-Target Correlation\n",
        "\n",
        "Third-party features google_index and page_rank dominate the correlations, at about 0.74 and 0.50 respectively.  However, url features nb_www (corr=0.43) and ratio_digits_url (corr=0.36) also look promising, and they're closely followed by site content variables domain_in_title (corr=0.33) and nb_hyperlinks (corr=0.33).\n",
        "\n",
        "On the other hand, most features show very little correlation, including several under 0.01.  And six features show no variation at all after removing the validation set from the original training set. (This was previously stable over a period of days at five, but is now stable at six. It's unclear why this changed, as the random seed and feature transformations have not changed).\n",
        "\n",
        "Weakly-correlated features could still plausibly be useful to decision tree models, and will be retained at this stage.  \n",
        "\n",
        "However, the following features do not vary in the training set can't be of any use, and will be immediately dropped from further analysis or modeling efforts:\n",
        "* nb_or,\n",
        "* ratio_nullHyperlinks,\n",
        "* ratio_intRedirection,\n",
        "* ratio_intErrors,\n",
        "* submit_email,\n",
        "* sfh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9US3ozmHoS4"
      },
      "outputs": [],
      "source": [
        "# Recombine X,y training data sets for exploration\n",
        "Xy_train = X_train.copy()\n",
        "Xy_train['y'] = y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pquoX8SGazEh"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "corr_matrix = Xy_train.corr().abs()\n",
        "\n",
        "# Get features most and least correlated with target variable\n",
        "print(corr_matrix['y'].sort_values(ascending=False).head(45))\n",
        "print(corr_matrix['y'].sort_values(ascending=False).tail(45))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YLz3b1aDam8"
      },
      "outputs": [],
      "source": [
        "# Identify non-varying features\n",
        "non_varying_droplist = []\n",
        "for col in X_train.columns:\n",
        "  if len(pd.unique(X_train[col])) < 2:\n",
        "    non_varying_droplist.append(col)\n",
        "\n",
        "print(non_varying_droplist)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Drop non-varying features"
      ],
      "metadata": {
        "id": "wYJJLrexbdLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyFvjZQpDk35"
      },
      "outputs": [],
      "source": [
        "# Add non-varying features to the drop list & drop from X_train\n",
        "\n",
        "X_train.drop(columns=non_varying_droplist, inplace=True)\n",
        "X_validation.drop(columns=non_varying_droplist, inplace=True)\n",
        "Xy_train.drop(columns=non_varying_droplist, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEASoVxF1HzQ"
      },
      "source": [
        "### Initial parallel coordinate plots on correlated feature groups\n",
        "\n",
        "I generated several groups of correlated features and plotted those groups (with the target variable) in parallel coordinate plots.\n",
        "\n",
        "This exposed interesting relationships that couldn't be seen in the correlation matrix.  For example, tld_in_subdomain = 1 in only about 5% of observations, but the vast majority of these are phishing.  In a similar vein, ip = 1 accounts for about 15% of observations, of which about 5/6 are phishing.  Each of these could potentially serve as a red flag for phishing, even if they don't provide much information about most of the dataset.   \n",
        "\n",
        "We can also see somewhat less narrowly-applicable features.  For example, setting a threshold of ratio_digits_url >= 0.05 gives us a little over 30% of observations, and of these almost 3/4 are phishing.  At nb_dots >= 4, we select about 13% of observations, nearly all phishing, but at nb_dots >= 3, this increases to about 33% of observations, which which almost 2/3 are phishing.\n",
        "\n",
        "This pattern continues across numerous features: they may serve as red flags at higher thresholds, or may provide useful correlation at lower thresholds, or in some cases they may do both.\n",
        "\n",
        "In addition to numerous features in these useful categories, a handful of features continued to show very strong potential. The google_index feature could form a phishing model by itself, and page_rank and domain_age each carry a lot of useful information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLunKbmUgrWm"
      },
      "source": [
        "##### Group correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-mxEeLymUc2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Create empty dataframes\n",
        "dfs_eda = []\n",
        "for i in range(0,13):\n",
        "  df = pd.DataFrame()\n",
        "  dfs_eda.append(df)\n",
        "\n",
        "form_group_threshold = 0.3\n",
        "join_group_threshold = 0.15\n",
        "\n",
        "\n",
        "# Add the target variable to the first data frame to ensure\n",
        "# highly-correlated variables will be explored\n",
        "dfs_eda[0]['y'] = y_train\n",
        "\n",
        "# Iterate through columns prioritizing those most correlated with the target\n",
        "# grouping them based on correlation with columns already assigned to groups,\n",
        "# correlation with the most-correlated other column, the presence of empty\n",
        "# groups, and correlation thresholds.\n",
        "for col in Xy_train:\n",
        "\n",
        "  #Get the next most-correlated column other than col itself\n",
        "\n",
        "  most_correlated = corr_matrix[col].sort_values(ascending=False).index[1]\n",
        "  correlation = corr_matrix.loc[col, most_correlated]\n",
        "\n",
        "  # Check if there are any remaining empty groups\n",
        "  empty_remaining = False\n",
        "  for df in dfs_eda:\n",
        "    if df.empty:\n",
        "      empty_remaining = True\n",
        "\n",
        "  # If there are empty remaining groups and correlation for the current column\n",
        "  # exceeds the threshold for group formation, add both to the first empty df\n",
        "  if empty_remaining and correlation > form_group_threshold:\n",
        "    for df in dfs_eda:\n",
        "      if df.empty:\n",
        "        df[col] = Xy_train[col]\n",
        "        df[most_correlated] = Xy_train[most_correlated]\n",
        "        break\n",
        "  # Otherwise, if the correlation exceeds the minimum threshold for joining a\n",
        "  # group\n",
        "  elif corr_matrix['y'][col] > 0.3:\n",
        "    dfs_eda[0][col] = Xy_train[col]\n",
        "  elif correlation > join_group_threshold:\n",
        "    for df in dfs_eda:\n",
        "      if most_correlated in df.columns:\n",
        "        df[col] = Xy_train[col]\n",
        "        break\n",
        "  elif corr_matrix['y'][col] > 0.15:\n",
        "    dfs_eda[0][col] = Xy_train[col]\n",
        "\n",
        "\n",
        "# Add the target variable to each group\n",
        "for df in dfs_eda:\n",
        "  if not 'y' in df.columns:\n",
        "    df['y'] = y_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnwxuOD-guLU"
      },
      "source": [
        "##### Generate parallel coordinate plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgHCZGmcVPyy"
      },
      "outputs": [],
      "source": [
        "import hiplot as hip\n",
        "\n",
        "# convert dfs_eda to list of dicts because hiplot requires\n",
        "dicts_eda = [df.to_dict('records') for df in dfs_eda]\n",
        "\n",
        "for d in dicts_eda:\n",
        "  hip.Experiment.from_iterable(d).display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02JS_WFUUavy"
      },
      "source": [
        "### Baseline Models\n",
        "\n",
        "Simple logistic regression and random forest models are run at this stage to provide baseline models prior to assessing feature importance. Preprocessing consists only of sklearn's Standard Scaler at this stage.  Other scalers will be assessed later.\n",
        "\n",
        "On the validation set, using all remaining features, two simple baseline models were run:\n",
        "* **Logistic regression model**: **ROC-AUC > 0.94**\n",
        " * Iterations limit increased to 115 (default 100) to allow convergence\n",
        " * Overfitting concerns minimal\n",
        "* **Random forest model**: **ROC-AUC > 0.95**\n",
        " * Estimators limited to 75 (default 100)\n",
        " * Tree depth limited to 7 (default none)\n",
        " * Both parameters selected to simplify model and avoid overtraining\n",
        " * Overfitting concerns minimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhWIcYQBan73"
      },
      "source": [
        "##### Model performance evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoOCQmWjs5lL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Define model evaluation function\n",
        "\n",
        "def print_model_evaluation(\n",
        "    title: str,\n",
        "    pipe : Pipeline,\n",
        "    X : pd.DataFrame,\n",
        "    y : pd.DataFrame):\n",
        "\n",
        "    print(\"\\n\" + title)\n",
        "    pred_y = pipe.predict(X)\n",
        "\n",
        "    print(\"\\nROC-AUC: \" + str(roc_auc_score(pred_y, y)))\n",
        "    print(\"Precision: \" + str(precision_score(pred_y, y)))\n",
        "    print(\"Recall: \" + str(recall_score(pred_y, y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs8IoTZHHce0"
      },
      "source": [
        "##### **Logistic regression** model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-vSb9zqHbyc"
      },
      "outputs": [],
      "source": [
        "# Set up pipeline\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_lr = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      LogisticRegression(\n",
        "          random_state=random_seed,\n",
        "          max_iter = 115 #Iterations increased to achieve convergence\n",
        "          )\n",
        ")\n",
        "\n",
        "pipe_lr.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or1lEMszW3u9"
      },
      "outputs": [],
      "source": [
        "print_model_evaluation(\"Logistic Regression\\nPerformance on Training Set\",\n",
        "                       pipe_lr, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Logistic Regression\\nPerformance on Validation Set\",\n",
        "                       pipe_lr, X_validation, y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rPGTjm8W44U"
      },
      "source": [
        "##### **Random forest** model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDtdj6L7W4PZ"
      },
      "outputs": [],
      "source": [
        "# Set up & run pipeline - random forest\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_rf = make_pipeline(\n",
        "      QuantileTransformer(),\n",
        "      RandomForestClassifier(\n",
        "          n_estimators=75,\n",
        "          max_depth=7,\n",
        "          random_state=random_seed\n",
        "          )\n",
        ")\n",
        "\n",
        "pipe_rf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vr4PsqKW5zA"
      },
      "outputs": [],
      "source": [
        "print_model_evaluation(\"Random Forest\\nPerformance on Training Set\",\n",
        "                       pipe_rf, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Random Forest\\nPerformance on Validation Set\",\n",
        "                       pipe_rf, X_validation, y_validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdxQc9wfHXXk"
      },
      "source": [
        "### Feature Importance Analysis\n",
        "\n",
        "**Preprocessing applied**: At this point, invalid values in domain_registration_length and domain_age were converted to NaN\n",
        "\n",
        "**Model for feature importance analysis**: Initial assessments of feature importance for features selection were performed by generating a simple histogram-based gradient-boosted tree classifier.\n",
        "\n",
        "**Permuation importance**: Sklearn's permutation importance was used to determine feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Convert invalid values"
      ],
      "metadata": {
        "id": "XcZvBt0vbM6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert invalid values to show they are missing\n",
        "import numpy as np\n",
        "X_train[\"domain_registration_length\"] = [np.NaN if x < 0 else x for x in X_train[\"domain_registration_length\"]]\n",
        "X_validation[\"domain_registration_length\"] = [np.NaN if x < 0 else x for x in X_validation[\"domain_registration_length\"]]\n",
        "\n",
        "X_train['domain_age'] = [np.NaN if x < 0 else x for x in X_train['domain_age']]\n",
        "X_validation['domain_age'] = [np.NaN if x < 0 else x for x in X_validation['domain_age']]"
      ],
      "metadata": {
        "id": "ZKkphBypbJsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-bFzlbyW4Ad"
      },
      "source": [
        "##### Train histogram gradient-boosted tree model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSMt4Oa8W6pk"
      },
      "outputs": [],
      "source": [
        "# Set up and run pipeline - gradient boosted trees\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_gbt = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      HistGradientBoostingClassifier(\n",
        "          max_iter = 60,\n",
        "          max_depth = 7,\n",
        "          random_state = random_seed)\n",
        ")\n",
        "\n",
        "pipe_gbt.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn-huRZLW6c4"
      },
      "outputs": [],
      "source": [
        "print_model_evaluation('Gradient-boosted Trees\\nPerformance on Training Set',\n",
        "                       pipe_gbt, X_train, y_train)\n",
        "\n",
        "print_model_evaluation('Gradient-boosted Trees\\nPerformance on Validation Set',\n",
        "                       pipe_gbt, X_validation, y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDxftzc3UVCp"
      },
      "source": [
        "##### Permutation importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb6SL1v_Mrp4"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "r = permutation_importance(pipe_gbt, X_validation, y_validation,\n",
        "                           n_repeats=100,\n",
        "                           random_state = random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As0gPAsAQ5ZE"
      },
      "outputs": [],
      "source": [
        "perm_imp = pd.DataFrame()\n",
        "perm_imp['feature'] = X_validation.columns\n",
        "perm_imp['importances_mean'] = r.importances_mean\n",
        "perm_imp['importances_std'] = r.importances_std\n",
        "perm_imp['importances_mean_less_std'] = r.importances_mean - r.importances_std\n",
        "perm_imp = perm_imp.sort_values(by='importances_mean', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J_aJ4uAbk9L"
      },
      "outputs": [],
      "source": [
        "perm_imp['importances_mean_less_std'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Hbyko2SKGI"
      },
      "outputs": [],
      "source": [
        "perm_imp.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JMo8wQkcRA_"
      },
      "outputs": [],
      "source": [
        "perm_imp = perm_imp.sort_values(by='importances_mean_less_std', ascending=False)\n",
        "perm_imp.head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmdNSbTMiTRk"
      },
      "source": [
        "### Feature Selection for Main Modeling Effort\n",
        "\n",
        "Feature importance was assessed using the validation set and sklearn's permutation importance function.  This analysis confirmed some previous suspicions, with only small surprises: google_index and page_rank dominated (expected) but very close together (somewhat unexpected). Just two other features (nb_www and nb_hyperlinks) showed mean importance >0.01, and a total of 19 showed mean importance >0.001.  \n",
        "\n",
        "Feature importance varied widely.  Just 14 features show importance_mean-importance_std > 0 with importance_mean > 0.001. We can be reasonably confident that these features carry at least some useful information most of the time.\n",
        "\n",
        "These 14 features will be used for the main modeling effort; all others will be rejected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = perm_imp[perm_imp['importances_mean_less_std'] > 0.0]\n",
        "selected_features = selected_features[selected_features['importances_mean'] > 0.001]\n",
        "print(selected_features['feature'])"
      ],
      "metadata": {
        "id": "KG6zasUrrhmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}